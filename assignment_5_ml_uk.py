# -*- coding: utf-8 -*-
"""Assignment_5_ML_UK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17bYs0NoaLlGpjwdViaed4kSL77uKk_uS

# **1. Principal Component Analysis**
a. Apply PCA on CC dataset.

b. Apply k-means algorithm on the PCA result and report your observation if the silhouette score has improved or not?

c. Perform Scaling+PCA+K-Means and report performance.
"""

# Commented out IPython magic to ensure Python compatibility.
#import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC  
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
from sklearn import metrics
from sklearn.metrics import accuracy_score, classification_report
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import seaborn as sns
# %config InlineBackend.figure_format='retina'

##Created a dataset named as cc_dataset with data provided in CSV file ‘datasets/CC.csv’.
#Head() method returns the first n rows from the dataframe.
dataset = pd.read_csv('CC.csv')
dataset.keys()

dataset.shape

#Applying k-means algorithm on the PCA result 
#The SimpleImputer class provides basic strategies for imputing missing values
X = dataset.iloc[:,1:]
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer = imputer.fit(X)
X = imputer.transform(X)
X=pd.DataFrame(X)

pca = PCA(2)
x_pca = pca.fit_transform(X)
df2 = pd.DataFrame(data=x_pca)
finaldf = pd.concat([df2, X.iloc[:,-1]], axis=1)
finaldf.head()

#Data Analytics
dataset = dataset.iloc[:,1:]
dataset['MINIMUM_PAYMENTS'] = dataset['MINIMUM_PAYMENTS'].fillna(np.mean(dataset['MINIMUM_PAYMENTS']))
dataset['CREDIT_LIMIT'] = dataset['CREDIT_LIMIT'].fillna(np.mean(dataset['CREDIT_LIMIT']))
dataset.isnull().sum()

# Standardize the data to have a mean of ~0 and a variance of 1
X_std = StandardScaler().fit_transform(dataset)

# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_ratio_, color='black')
plt.xlabel('PCA features')
plt.ylabel('variance %')
plt.xticks(features)

# Saving in components to a DataFrame
PCA_components = pd.DataFrame(principalComponents)

"""Fig above shows that the first two components explain the majority of the variance in our data."""

plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')

ks = range(1, 10)
inertias = []
for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(PCA_components.iloc[:,:3])
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
plt.plot(ks, inertias, '-o', color='black')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()

"""# **1.B. Apply k-means algorithm on the PCA result and report your observation if the silhouette score has improved or not?**"""

#Read Data Set
df = pd.read_csv('CC.csv')

x = df.iloc[:,[1,2,3,4]]
y = df.iloc[:,-1]

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
df['CUST_ID'] = le.fit_transform(df.CUST_ID.values)

pca2 = PCA(n_components=2)
principalComponents = pca2.fit_transform(x)

principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])

finalDf = pd.concat([principalDf, df[['TENURE']]], axis = 1)
finalDf.head()

from sklearn.cluster import KMeans
nclusters = 2 #the k in kmeans
km = KMeans(n_clusters=nclusters)
km.fit(x)

# predicting the cluster for each data point
y_cluster_kmeans = km.predict(x)
from sklearn import metrics
score = metrics.silhouette_score(x, y_cluster_kmeans)
print(score)

##Calculating the silhouette score for the above clustering
scaler = StandardScaler()
X_Scale = scaler.fit_transform(x)

pca2 = PCA(n_components=2)
principalComponents = pca2.fit_transform(X_Scale)

principalDf1 = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])

finalDf1 = pd.concat([principalDf1, df[['TENURE']]], axis = 1)
finalDf1.head()

from sklearn.cluster import KMeans
nclusters = 2 #the k in kmeans
km = KMeans(n_clusters=nclusters)
km.fit(X_Scale)

# predicting the cluster for each data point
y_cluster_kmeans = km.predict(X_Scale)
from sklearn import metrics
score = metrics.silhouette_score(X_Scale, y_cluster_kmeans)
print(score)

"""# **1.C. Perform Scaling+PCA+K-Means and report performance.**"""

#Head() method returns the first n rows from the dataset.
dataset.head()

dataset.shape

dataset['TENURE'].value_counts()

X = dataset.drop('TENURE',axis=1).values
y = dataset['TENURE'].values

scaler = StandardScaler()
X_Scale = scaler.fit_transform(X)

pca2 = PCA(n_components=2)
principalComponents = pca2.fit_transform(X_Scale)

principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])

finalDf = pd.concat([principalDf, dataset[['TENURE']]], axis = 1)
finalDf.head()

X = finalDf.drop('TENURE',axis=1).values
y = finalDf['TENURE'].values

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=0)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# logisticRegr = LogisticRegression()
# logisticRegr.fit(X_train,y_train)
# 
# y_train_hat =logisticRegr.predict(X_train)
# train_accuracy = accuracy_score(y_train, y_train_hat)*100
# print('"Accuracy for our Training dataset with PCA is: %.4f %%' % train_accuracy)

"""# **2. Use pd_speech_features.csv**
# **a. Perform Scaling**
# **b. Apply PCA (k=3)**
# **c. Use SVM to report performance**
"""

##Created a dataframe named as speech_df with data provided in CSV file 'pd_speech_features.csv’.
#Head() method returns the first n rows from the dataframe.
speech_df=pd.read_csv('pd_speech_features.csv')
speech_df.head()

#Data Analytics
x =speech_df.iloc[:,1:]
scaler = StandardScaler()
scaler.fit(x)
speech_x_scaler = scaler.transform(x)
print(speech_x_scaler)

pca = PCA(3)
speech_x_pca = pca.fit_transform(speech_x_scaler)
speech_df2 = pd.DataFrame(data=speech_x_pca)
speech_finaldf = pd.concat([speech_df2,speech_df[['class']]],axis=1)
print(speech_finaldf)

#splitting the data into test and train data to calculate svm accuracy
clf = SVC(kernel='linear') 

x =speech_finaldf.iloc[:,:-1]
y =speech_finaldf.iloc[:,-1]

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)

clf.fit(X_train, y_train)
y_pred=clf.predict(X_test)
accuracy_score(y_test, y_pred)

print("svm accuracy =", accuracy_score(y_test, y_pred))

#Report Performance
print(classification_report(y_test, y_pred))



"""# **3. Apply Linear Discriminant Analysis (LDA) on Iris.csv dataset to reduce dimensionality of data to k=2.**"""

# importing required packages
np.set_printoptions(precision=4)

#Created a dataframe named as df with data provided in CSV file 'Iris.csv’.
#Head() method returns the first n rows from the dataframe
df = pd.read_csv("Iris.csv")
df.head()

#Standardize features
from sklearn.preprocessing import StandardScaler
stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(df.iloc[:,range(0,4)].values)

class_le = LabelEncoder()
y = class_le.fit_transform(iris_df['Species'].values)

lda = LinearDiscriminantAnalysis(n_components=2)
X_train_lda = lda.fit_transform(X_train_std,y)

data=pd.DataFrame(X_train_lda)
data['class']=y
data.columns=["LD1","LD2","class"]
data.head()

markers = ['s', 'x', 'o']
colors = ['r', 'b', 'g']
sns.lmplot(x="LD1", y="LD2", data=data, hue='class', markers=markers, fit_reg=False, legend=False)
plt.legend(loc='upper center')
plt.show()

# Constructing within-class covariant scatter matrix S_W
S_W = np.zeros((4,4))
for i in range(3):
    S_W += np.cov(X_train_std[y==i].T)

S_W

#Construct between-class scatter matrix S_B
N=np.bincount(y) # number of samples for given class
vecs=[]
[vecs.append(np.mean(X_train_std[y==i],axis=0)) for i in range(3)] # class means
mean_overall = np.mean(X_train_std, axis=0) # overall mean
S_B=np.zeros((4,4))
for i in range(3):
    S_B += N[i]*(((vecs[i]-mean_overall).reshape(4,1)).dot(((vecs[i]-mean_overall).reshape(1,4))))

S_B

#Calculating sorted eigenvalues and eigenvectors of inverse(S_W)dot(S_B)
eigen_vals, eigen_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))
eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) for i in range(len(eigen_vals))]
eigen_pairs = sorted(eigen_pairs,key=lambda k: k[0], reverse=True)
print('Eigenvalues in decreasing order:\n')
for eigen_val in eigen_pairs:
    print(eigen_val[0])

#Ploting of the main LDA components
tot = sum(eigen_vals.real)
discr = [(i / tot) for i in sorted(eigen_vals.real, reverse=True)]
cum_discr = np.cumsum(discr)
plt.bar(range(1, 5), discr, width=0.2,alpha=0.5, align='center',label='individual "discriminability"')
plt.step(range(1, 5), cum_discr, where='mid',label='cumulative "discriminability"')
plt.ylabel('"discriminability" ratio')
plt.xlabel('Linear Discriminants')
plt.ylim([-0.1, 1.1])
plt.legend(loc='best')
plt.show()

"""# **4. Briefly identify the difference between PCA and LDA**

1.Both LDA and PCA are methods of linear transformation.

2.LDA maximizes the distance between various classes, whereas PCA maximizes the variance of the data.

3.While PCA is an unsupervised method, LDA is supervised.

4.The transform method for LDA requires the X train and the Y train as two parameters. The transform method just needs one parameter, X train, when it comes to PCA. This illustrates how PCA does not rely on the output labels, whereas LDA considers the output class labels for choosing the linear discriminants.

5.When there are fewer samples in each class, PCA performs better. LDA, on the other hand, performs better on large datasets with many classes.
"""